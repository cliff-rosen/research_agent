# Workflow

## Core Analysis Components
- Question decomposition into atomic claims/queries
- Identification of implicit assumptions
- Definition of scope boundaries
- Success criteria specification
- Stakeholder perspective mapping

## Key Behaviors
- Creates quantitative scoring systems for each major aspect
- Establishes clear thresholds for iteration triggers
- Enables tracking of progress across all dimensions
- Creates explicit quality gates that must be passed
- Ties back to the original atomic claims throughout

## Workflow Steps
| Step | Name | Input | Output | Description |
|------|------|--------|---------|-------------|
| 1 | Question Receipt & Documentation | • Raw question from user | • Documented question<br>• Initial metadata including:<br>  - Question timestamp<br>  - Domain classification<br>  - Urgency level<br>  - Required confidence level | Capture and classify the question with standardized metadata tags for tracking and prioritization |
| 2A | Atomic Decomposition | • Documented question | • List of atomic claims/queries<br>• Dependencies between claims<br>• Minimum evidence requirements per claim | Break down complex questions into individual testable claims, establishing evidence requirements for each |
| 2B | Evaluation Framework Generation | • Atomic claims list<br>• Domain classification | • Source Quality Rubric (1-5 scale)<br>• Information Extraction Checklist<br>• Conflict Resolution Framework<br>• Answer Completeness Metrics<br>• Required confidence thresholds | Create quantitative evaluation frameworks that will be used throughout the process |
| 2C | Success Criteria Definition | • Atomic claims<br>• Evaluation frameworks | • Required scores for each metric<br>• Minimum source requirements<br>• Coverage thresholds<br>• Confidence level targets<br>• Iteration trigger conditions | Define specific, measurable criteria that must be met for each atomic claim |
| 3 | Query Strategy Development | • Atomic claims<br>• Evaluation frameworks | • Search strategy per claim including:<br>  - Primary search terms<br>  - Alternative phrasings<br>  - Required source types<br>  - Coverage tracking matrix | Design comprehensive search strategies that ensure adequate coverage of all claims and perspectives |
| 4 | Source Collection & Scoring | • Search strategies<br>• Source Quality Rubric | • Scored source database with:<br>  - Authority score (1-5)<br>  - Recency score (1-5)<br>  - Methodology score (1-5)<br>  - Coverage map to atomic claims<br>  - Cumulative quality score | Identify and evaluate sources using the predefined quality rubric, mapping them to specific claims |
| 5 | Information Extraction & Classification | • Scored sources<br>• Information Extraction Checklist | • Evidence database with:<br>  - Tagged evidence snippets<br>  - Evidence type classification<br>  - Quality scores<br>  - Source context<br>  - Claim mapping<br>  - Confidence scores | Extract and classify evidence using standardized formats and scoring systems |
| 6 | Evidence Evaluation | • Evidence database<br>• Evaluation frameworks | • Evidence strength assessment<br>• Coverage gaps analysis<br>• Conflict identification<br>• Uncertainty quantification<br>• Quality scores per claim | Analyze collected evidence against predefined quality metrics, identifying gaps and conflicts |
| 7 | Conflict Resolution Processing | • Evidence evaluation<br>• Conflict Resolution Framework | • Resolved conflicts with:<br>  - Resolution rationale<br>  - Confidence scores<br>  - Supporting evidence<br>  - Remaining uncertainties<br>  - Alternative viewpoints | Apply structured resolution process to conflicts, documenting rationale and remaining uncertainties |
| 8 | Initial Answer Assembly | • Resolved evidence<br>• Answer Completeness Metrics | • Draft answer with:<br>  - Evidence mapping<br>  - Confidence scores<br>  - Gap analysis<br>  - Quality metrics<br>  - Uncertainty documentation | Construct initial answer ensuring all completeness metrics are addressed |
| 9 | Quality Control Review | • Draft answer<br>• Success criteria<br>• All evaluation frameworks | • Compliance report showing:<br>  - Criteria met/unmet<br>  - Score by dimension<br>  - Gap analysis<br>  - Improvement requirements | Evaluate answer against all success criteria and quality metrics |
| 10 | Targeted Refinement | • Quality control report<br>• Improvement requirements | • Refinement plan with:<br>  - Required iterations<br>  - Specific targets<br>  - Success metrics<br>  - Timeline | Plan specific improvements needed to meet quality thresholds |
| 11 | Final Production | • Refined answer<br>• All quality documentation | • Final answer package:<br>  - Complete answer<br>  - Evidence summary<br>  - Quality scores<br>  - Confidence levels<br>  - Resolution documentation<br>  - Uncertainty statement | Produce final deliverable with all supporting documentation and quality metrics |

### Generated Evaluation Checklists

#### A. Source Quality Checklist
Score each source (1-5) on:
- Authority (credentials, reputation, expertise level)
- Recency (publication date relevance)
- Methodology rigor
- Data completeness
- Peer review status
- Conflicts of interest
- Citation network strength

#### B. Information Extraction Checklist
For each extracted information nugget, evaluate:
- Direct relevance to atomic claims (1-5)
- Evidence type classification
  - Primary research
  - Secondary analysis
  - Expert opinion
  - Anecdotal evidence
- Contextual completeness (1-5)
- Methodology transparency (1-5)
- Statistical significance (where applicable)
- Effect size (where applicable)

#### C. Conflict Resolution Rubric
For each identified conflict:
- Evidence strength differential
- Methodology comparison
- Sample size/scope differences
- Timeline considerations
- Definition/terminology disparities
- Geographic/cultural context variations
- Stakeholder bias assessment

#### D. Answer Completeness Checklist
Verify:
- All atomic claims addressed
- Evidence provided for each claim
- Uncertainty levels specified
- Alternative viewpoints represented
- Methodology limitations acknowledged
- Context adequately explained
- Assumptions explicitly stated
- Scope boundaries respected

### 2.3 Progress Tracking Metrics

For each subsequent step, track:
- Completion percentage of relevant checklists
- Average scores on quality metrics
- Number of unresolved conflicts
- Coverage of identified stakeholder perspectives
- Information gaps remaining
- Uncertainty levels by claim

### 2.4 Iteration Triggers

Define specific thresholds that trigger return to previous steps:
- Source quality scores below 3/5
- Information gaps in critical areas
- Unresolved high-impact conflicts
- Stakeholder perspective coverage below 80%
- Answer completeness score below 4/5

### 2.5 Final Quality Gates

Establish pass/fail criteria for:
- Minimum source quality averages
- Maximum acceptable uncertainty levels
- Required stakeholder perspective coverage
- Essential context inclusion
- Evidence strength thresholds
- Conflict resolution completeness

## Integration with Other Steps

### Step 3: Query Expansion
- Use atomic claims to generate claim-specific search terms
- Map stakeholder terminology variations
- Track coverage of success criteria

### Step 4: Source Identification
- Apply source quality checklist
- Track authority coverage gaps
- Map sources to atomic claims

### Steps 5-6: Information Extraction & Analysis
- Apply extraction checklist to each nugget
- Track evidence type distribution
- Monitor claim coverage

### Step 7: Conflict Resolution
- Apply resolution rubric to each conflict
- Track resolution progress
- Document resolution rationale

### Steps 8-11: Answer Production & Refinement
- Use completeness checklist for evaluation
- Track quality metrics through iterations
- Document threshold compliance